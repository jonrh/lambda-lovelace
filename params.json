{
  "name": "Lambda-lovelace",
  "tagline": "A collaborative recommender system for tweets. A 30 ECTS group project, summer 2016 at University College Dublin.",
  "body": "# λ Lovelace\r\nA collaborative recommender system for tweets; a personalised tweet stream. Module: COMP47250, a 30 ECTS group project, summer 2016 at University College Dublin.\r\n\r\n## Project\r\n**Summary**: A collaborative recommender system for tweets; a personalised tweet stream.\r\n\r\nThe theme of 2016 for the group project module is *Future Of News*. The premise for our project is the asumption (or observation) that people are experiencing an information overload. Years or decades ago news or content creators were few (print, television, radio) compared to today. Now everyone with a computer or a smartphone can be a content creator. We believe that the future of news is going to be filtering and delivering personalised news to people. We see our project to be a stepping stone in that direction, starting with Twitter.\r\n\r\n### Scope\r\nOn a very high level there are two main components to the project:\r\n\r\n* **Front-end**: iOS Twitter mobile app\r\n* **Back-end**: Collaborative recommender system\r\n\r\nA user with a Twitter account signs up to our services and uses our iOS mobile client. Tweets from followers not of interest (e.g. political rants) are filtered out (or deferred to later) while interesting tweets are prioritised in the timeline. Tweets from non-followers may be suggested as well. Essentially we hope to create a better, more personalised timeline of tweets than what Twitter provides by default. Our iOS app will make observations of the users engagements (opening, liking, time in focus, etc) and sends the information to the recommender back-end for further recommendations.\r\n\r\nThe mobile app is required in order to collect additional user preference information to refine the recommendations. For example, if a user clicks a link in a tweet, likes a tweet, retweets, or engages in conversations. Another potential passive observation mechanishm would be to have the client measure the amount of time a tweet is visible. Thinking being if a tweet is in focus for longer it might be of more interest than a tweet that is scrolled past quickly.\r\n\r\nOur contributions or novelty if you will are as follows:\r\n\r\n1. Filter out uninteresting tweets (or defer to later)\r\n2. Collect additional user preferences in a mobile app\r\n3. Show interesting tweets from non-followers\r\n\r\nThese are ordered by priority, that is we will first strive to implement tweet filtering, then data collection in the mobile app and if things go well we will try to introduce outsider tweets that might be of interest to the user.\r\n\r\nIn the beginning our project scope was to create a general recommendation system for all sorts of media: news, tweets, blogs, videos, etc. However given professor's feedback we decided to focus the idea on Twitter. We believe that doing so will allow us to deliver a more refined and complete solution. However if things go exceedingly well we may revisit this idea for further expansion.\r\n\r\n## Minimum Viable Product\r\nThe minimum viable product we set ourselves out to achieve is a recommender system to filter out tweets from the timeline that are not of interest. That is, it will only include the back-end, no mobile app. The aim is to have a tweet timeline where a user would rate our version better than the default one provided by Twitter. We will see how deep we will get but the aim is to be able to have an objective evaluation that shows a statistically significant difference in user preference.\r\n\r\n\r\n## Evaluation Method\r\n\r\nThere are two evaluations that come to mind:\r\n\r\n- Quality / Accuracy of recommender system\r\n- Usability of mobile client\r\n\r\nA cornerstone of the project will be to filter and order tweets to the user in a personalised way superior to the default Twitter timeline (all tweets from followers). The recommender system can be evaluated in two ways:\r\n\r\n- **Evaluation on a static dataset**. Static datasets constructed from existing Twitter accounts. Something we can test over and over to benchmark ourselves. Sort of like unit tests. Try to predict likes for example. This evaluation is more intended to aid us during development.\r\n- **User evaluation**. Here are some ideas for user evaluations:\r\n  - Present pairs of unseen tweets from a users timeline. User selects which tweet is more interesting or relevant. Our recommender system would make it's prediction behind the scenes. We would then compare and see if the guess by the recommender system is correct or not.\r\n  - A user is presented 10 (or X number) unseen tweets from her or his timeline. The user is asked to place the tweets in order of interest. This order would be compared to the order the recommender system predicted.\r\n  - True evaluations from live users. This is more fuzzy, we have users try our system with their Twitter account and report how accurate the predictions are. A full user run so to speak.\r\n\r\nNot sure how (or if at all) we would test the usability of the mobile app, but there are probably well known ways to do it : )\r\n\r\n## Technical Decisions\r\nHere below are some of the technical descisions we've made so far. Please note that we do not consider them binding. That is, we are fully prepared to switch languages, stacks mid project if we believe it will suit us better.\r\n\r\n- **Mobile**: iOS 9 + Swift 2.2\r\n- **Recommender System**: Python 3\r\n- **Back-end web service**: Python 3 or 2, [Flask](http://flask.pocoo.org/) (or [Bottle](http://bottlepy.org/docs/dev/index.html))\r\n- **Database**: Undecided. Maybe [PostGres](https://www.postgresql.org/), [Redis](http://redis.io/), or [RethinkDB](http://rethinkdb.com/).\r\n\r\nFor the backend we'll strive to use Python 3 as much as we can but for some parts it may be nescisary to use Python 2.7. For the recommender system we aim to use Python 3 data scicence libraries as much as we can. However Python is not the fastest language on the block so we've pondered the possibility to dip into [Rust](https://www.rust-lang.org/) for performance critical parts, but we'll see.\r\n\r\nAs for the database we have not entirely made up our mind. What comes to mind is PostGres for general storage. The Twitter API has pretty restrictive rate limits so it looks like we might need to store tweets ourselves. What comes to mind are some document databases like Redis or RethinkDb.\r\n\r\n\r\n## Project Managment\r\n\r\nFor project managment we keep it loose & lean. We use [ZenHub](https://www.zenhub.io/) to augment GitHub so we get a Kanban style board for issues and burndown graphs to track milestone progresses.\r\n\r\nFor issues we use the following story point estimations:\r\n\r\n| Story Points | Description |\r\n|:------------:|:------------|\r\n| **1**        | ~30m easy work, e.g. testing for the other team |\r\n| **2**        | 1 - 2 hours of work, simple but requires effort |\r\n| **3**        | half a day of work |\r\n| **5**        | full day of work |\r\n| **8**        | 2 days of work, not easy |\r\n| **13**       | 3 - 5 days of work, very complex may require multiple people | \r\n\r\n\r\n### Schedule & Deliverables\r\n\r\n2016-05-17\t\tLecture 1 (10:00 - 16:00)  \r\n2016-05-24\t\tLecture 2 (13:00 - 16:00)  \r\n2016-05-31\t\tWeek 3 Lab (13:00 - 16:00)  \r\n**2016-06-10\tWeek 4: Project Plan**  \r\n2016-06-14\t\tWeek 5 Lab (13:00 - 16:00)  \r\n**2016-06-21\tMid-term presentations** (10:00 - 17:00)  \r\n**2016-06-24\tMid-term report**  \r\n2016-07-05\t\tWeek 7 Lab (13:00 - 16:00)  \r\n**2016-07-15\tUser evaluation report**  \r\n2016-07-19\t\tWeek 8 Lab (13:00 - 16:00)  \r\n2016-08-02\t\tWeek 9 Lab (13:00 - 16:00)  \r\n**2016-08-09\tFinal presentations** (10:00 - 17:00)  \r\n**2016-08-19\tFinal Report & Code**  \r\n\r\n\r\n|             |    M    |    T    |    W    |    T    |    F    |    S    |    S    | Month     |\r\n|------------:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|:----------|\r\n| **Week 1**  | 16      | 17      | 18      | 19      | 20      | 21      | 22      | May       |\r\n| **Week 2**  | 23      | 24      | 25      | 26      | 27      | 28      | 29      | May       |\r\n| **Week 3**  | 30      | *31*    | 1       | 2       | 3       | 4       | 5       | May/June  |\r\n| **Week 4**  | 6       | *7*     | 8       | 9       | **10**  | 11      | 12      | June      |\r\n| **Week 5**  | 13      | *14*    | 15      | 16      | 17      | 18      | 19      | June      |\r\n| **Week 6**  | 20      | **21**  | 22      | 23      | **24**  | 25      | 26      | June      |\r\n| **Week 7**  | 27      | *28*    | 29      | 30      | 1       | 2       | 3       | June/July |\r\n| **Week 8**  | 4       | *5*     | 6       | 7       | 8       | 9       | 10      | July      |\r\n| **Week 9**  | 11      | *12*    | 13      | 14      | **15**  | 16      | 17      | July      |\r\n| **Week 10** | 18      | *19*    | 20      | 21      | 22      | 23      | 24      | July      |\r\n| **Week 11** | 25      | *26*    | 27      | 28      | 29      | 30      | 31      | July      |\r\n| **Week 12** | 1       | *2*     | 3       | 4       | 5       | 6       | 7       | August    |\r\n| **Week 13** | 8       | **9**   | 10      | 11      | 12      | 13      | 14      | August    |\r\n| **Week 14** | 15      | 16      | 17      | 18      | **19**  | 20      | 21      | August    |\r\n\r\n**Bold**: Deliverable or presentation  \r\n*Italic*: Blog and show'n'tell",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}