---
layout: post
title:  Week 10 Blog - Caching Data
author: Xinqi Li
date:   2016-07-17 23:00:00 +0100
---

In this post we will provide an update on what λ Lovelace has been up to:

* Caching data
* Challenges & Next Tasks

### Caching Data

As mentioned before, the **rate limit** of Twitter API is a problem that we are always trying to solve. For home timeline, we are only allowed to send 15 request per 15 minutes, which means it will hit the rate limit quickly if a user keeps refreshing his/her timeline. Besides, we can only fetch up to 800 **newest** tweets of user's home timeline. Therefore, the tweets before the 800 tweets will never be fetched.

To solve the rate limit problem, we finally decided to use a database to cache the tweets. And our Flask server will get tweets from database directly instead of getting from Twitter API. 

In order to cache data, we were thinking about setting up a background task which runs 24/7 for each user. This task keeps fetching the incoming tweets of the user's home timeline and save it into the database. There are two ways of continously getting tweets, [Twitter Streaming API](https://dev.twitter.com/streaming/overview) and [Celery](http://www.celeryproject.org/).

#### Celery vs. Twitter Streamming API

- Twitter Streamming API

The [Streaming API](https://dev.twitter.com/streaming/overview) gives developers low latency access to stream of Tweet data. It provides three streams: [public stream](https://dev.twitter.com/streaming/public), [user stream](https://dev.twitter.com/streaming/userstreams) and [site stream](https://dev.twitter.com/streaming/sitestreams).

For this project, site stream will be the most suitable stream. Unfortunately, twitter has closed the stream, which means our only choice will be the user stream. The **user stream** is a stream that contains roughly all of the data corresponding with a single user’s view of Twitter. Once we start a stream for a user, let it run 24/7, then we can store all the data associated with the user.

Although the user stream sounds like a very good choice of data source, we found that the streaming API also has rate limit. According to twitter,

*Each Twitter account is limited to only a few simultaneous User Streams connections per OAuth application, regardless of IP. Once the per-application limit is exceeded, the oldest connection will be terminated.*

Twitter didn't specify the maximum number of connections allowed. But we found somebody was asking ["What is the maximum number of simultaneous user streams per application?"](https://twittercommunity.com/t/what-is-the-maximum-number-of-simultaneous-user-streams-per-application/8335) on Twitter's Developer Forums. According to the answer, 10~20 simultaneous user streams will be reasonable. But if we have hundreds of users, user stream will be completely inappropriate for this kind of use case.

As we don't want to introudce a new rate limit while we are trying to slove the old one. So we came up with the second solution - **Celery**.

- Celery - Distributed Task Queue

Celery is an asynchronous task queue/job queue based on distributed message passing.	It is focused on real-time operation, but supports scheduling as well. So we can use Celery to set up a background task without affecting other requests of our app. As it supports scheduling, we can set up a periodic task which keeps fetching data 24/7. The basic scenario is described as below:

The Celery server is running 24/7 and the periodic task is executed every 60 seconds. The reason why we are doing this is because the rate limit of twitter API is 15 requests per 15 minutes. So if we only use one request in a minute, it will avoid hitting the rate limit.

CODE:

```python
#config celery, the task 'add' will be executed every 60 seconds
app.conf.update(
                CELERYBEAT_SCHEDULE = {
                "add": {
                "task": "tasks.add",
                "schedule": timedelta(seconds=60),
                }, },
                )
                
@app.task
def add():
    
    #read tokens of all user's in the database
    tokens = read_tokens()
    
    #iteratively fetch tweets of each user
    #all tasks are async tasks, so will not affetc each other
    for item in tokens:
        get_tweet.delay(item)
        
#method to read all tokens of the users from database
def read_tokens():
    
    r.connect(host = 'ec2-52-51-162-183.eu-west-1.compute.amazonaws.com', port = 28015, db='lovelace', password = "marcgoestothegym").repl()
    tokens=r.db('lovelace').table('user_tokens').run()
    return tokens
```

So the task 'add' will be executed every 60 seconds. The 'add' task wiil first read all tokens from the table 'user_tokens' in the database. The 'user_tokens' table is like a central table which specifies whose tweets Celery should fetch and store. It stores all the tokens of the users who have currently logged into our app. Every time after a user login, the Flask server will insert the user's OAuth token into the table so Celery will start fetching the tweets of the user. If there is no need to fetch tweets for the user, for example if a user has logged out for a long time, we can simply delete the user's token and Celery will not fetch the user's tweets anymore.

After getting the tokens, it will iteratively use the tokens to fetch new tweets from the Twitter REST API and save the tweets into the database by calling the 'get_tweet' method. The .delay() means each 'get_tweet' task is executed asynchronously, so they won't affect each other.  

CODE:

```python
#get tweets
@app.task(bind=True)
def get_tweet(self, token):
    #connect to database
    r.connect(host='ec2-52-51-162-183.eu-west-1.compute.amazonaws.com', port=28015, db='lovelace', password="marcgoestothegym").repl()
    
    created_at = token['created_at']
    print(created_at)
    now = r.now().to_epoch_time().run()
    print(now - created_at)
    screen_name = token['screen_name']
    #check the time iterval between the time when token is inserted and the time when next celery task executes
    #if the interval is less than 64 seconds, we wait for another 64 seconds,this is to avoid sending two requests to the Twitter API in one minute
    #we can only send one request each 64 seconds
    if (now - created_at) >= 64:
        
        #authentication
        auth = tweepy.OAuthHandler(consumer_key=token['consumer_key'], consumer_secret=token['consumer_secret'])
        auth.set_access_token(token['access_token'], token['access_secret'])
        
        api = tweepy.API(auth)
        
        #fetch user's home timeline and insert it into database
        #here is an error handling, if the rate limit exceed, the task will be retried after 5 seconds
        try:
            #since_id is the id of the newest tweet of user's home timeline in the database
            since_id = r.db('lovelace').table('tweets').filter({'screen_name':'xinqili123'}).max('tweet_id').run()
            #only fetch the tweets whose ids are greater than the since_id, to avoid fetching duplicate tweets
            new_tweets = [tweet._json for tweet in api.home_timeline(count=200,since_id = since_id['tweet_id'])]
            #insert each tweet into database
            for item in new_tweets:
                r.db('lovelace').table('tweets').insert({'screen_name': screen_name,'tweet_id':item['id'], 'tweet': item}).run()
            #check rate limit remaining
            limit = api.rate_limit_status()
            return limit['resources']['statuses']['/statuses/home_timeline']
        except tweepy.RateLimitError as exc:
            raise self.retry(exc=exc,countdown=5,max_retries=10)
```

This solution is more like we are building a streaming API manually. But this solution has a maximum one minute latency. For example, if new tweets come immediately after a task was just executed. The new tweets will have to wait one minute until the next task execution to be stored into database. It means sometimes, the user will see the new tweets a little bit later than the Twitter offical app. We had a discussion about this, and finally we believed that the main purpose of our app is providing a better recommendation for the users, so one miniute latency is within accptable range.

### Challenges & Next Tasks

- **Recommender System**: ???
- **iOS App**: ???
- **Continuous Deployment**: ???
- **Flask API**: ???

Until next time!

On behalf of λ Lovelace  
\- *Xinqi Li*